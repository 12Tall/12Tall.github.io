---
title: 我理解的神经网络
---
## 前言  
我一直希望，能不能有一种非常非常简单、形象的方式，来向人们解释复杂的理论。最好是能用生动的比喻、连同它的历史一起、娓娓道来。这种方式一定非常适合人们对事物的理解的习惯吧。

也许这是一个死胡同，走到某一步便走不下去了。也许这也是前人走过无数遍的道路，只不过谁也不曾注意到路边的野花。我更愿意说这是一种思维方式，中间掺杂着我大学以来学习、思考的结果。边走边看吧。

## 函数与黑盒子  
让我们回到初中的数学课堂，回忆一下什么是函数。大概的定义是一组数据到另一组数据的映射。但是以一个未知的概念去定义另一个未知的概念是一件很危险的事情：什么是映射呢？

### 单输入单输出函数  
不妨让我们将函数看作一个黑盒子，包含一个数据入口和一个数据出口的黑盒子。盒子内部会进行一系列的加工处理工作，比如：将输入的数据值增加1，并输出到外界。那我们就可以将这个函数的功能用数学公式表示：对于输入`x`，经过函数的处理后会输出`y=x+1`。好巧不巧，函数的英文名字是`function`，也刚好有“功能”的意思。  
![单输入单输出函数](./img/01_function/1in1out.svg)

### 多输入多输出函数  
更进一步，我们可以定义另一个盒子，包含两个输入`x,y`和两个输出`z1,z2`，其中`z1=x+y; z2=x-y`。当`y` 的值被锁定为`1` 时，`z1=x+1` 退化为上文提到的[单输入单输出函数](#单输入单输出函数)。  
![多输入多输出函数](./img/01_function/2in2out.svg)  
多输入单输出的函数在生活中也比较常见，比如：安全要求比较高的门禁系统，要同时插入两把钥匙才能打开；计算机中的调色板通过设置`R,G,B` 的值来唯一确定一种颜色；听到下课铃并且到饭点了才能去食堂吃饭。想一想，现实生活中可以总结出规律的事情，基本都能用一个多输入的黑盒子模块表示。

一般来说我们设计系统时，要求尽量做到模块化，新增的模块（黑盒子）对原有的系统不要有侵入（即不要影响本来系统的函数式），这样的模块能够做到即插即用。对于每一个模块而言，需要尽量做到输出对于输入没有影响，就算有，我们也能通过传递函数化简（概念取自于自动控制原理），将系统在概念层面化简为新的独立的模块构成。

## 统计与拟合  
自然界中一般会存在两种函数：已知的和未知的。像电压-电流关系`U=I*R`、牛顿第二定律`F=ma`、单位圆上的点（与角/弧度相关）到`x` 轴的投影`y=cos(x)`是已知的函数，我们能直接写出它们的函数式；另一种如股票价格变动、声音的波形、汽车油耗随速度的变化，不排除它们按某些规律变化，但是由于影响的因素过多，我们难以找到一个精确的函数式来表示他们的变化。
对于不知道确切函数式的函数，如果我们想知道某一段输入对应着输出变化的规律则需要用到统计学的方法了。这里的统计学并不包括概率，因为概率的概念还是太抽象了点。

### 散点图  
如果我们想要看到某段输入内，输出的变化趋势。我们可以将所有的输入输出点记录下来，如果记录的点足够多、分布足够均匀，就能得到较为精确的函数图像。实际上计算机作图也是通过这个原理，当离散的点足够密集时，就可以看成一条连续的线了。  
![散点图](./img/02_statistic/scatter-plot.svg)

### 插值法  
自然界中的信号，一般都是连续变化的，不会突然产生跳变。这就说明，对于两次间隔比较小的输入，其输出一般也差距较小。于是，对于某一个输入值`x1`，可以说其对应的输出取`y1=(y0+y2)/2`。这便是最简单的插值法。 
![一元线性插值](./img/02_statistic/linear-interpolation.svg)  
 

### 最小二乘法  
插值法可以较快地求取某一点的函数值，但是其受采样点的影响比较大，对多输入函数的效果不理想。最重要的是，我们想要一个通用的函数式来表示某个模块的功能。这时候我们就可以根据[散点图](#散点图)中的函数图像，来假设一个近似的函数方程`f(x)`：参数的系数不固定，通过计算采样点到`f(x)`的竖直（比较好算）距离的最小值，来唯一确定一组系数，进而得到一条相对比较理想的曲线来拟合模块的函数式。这便是最小二乘法。  
![最小二乘法](./img/02_statistic/least-squares-method.svg)  

以上图为例，采样点分别为：`(x,y)∈{(0,1),(2,2.5),(3,4.5),(4,5),(5,6)}`，我们需要假设直线`y=kx+b`。则某一个采样点`(xi,yi)`到该直线的竖直距离就是`di=|yi-y(xi)|`。为了便于计算，我们可以用平方去掉绝对值符号，求所有`di²`的和`D`的最小值。这里我们可以用`Python`的`Sympy`库来帮助求解得到`D`关于`(k,b)`变化的函数；或者直接利用`Matlab`求出相应的`(k,b)`。   

<CodeGroup>
<CodeGroupItem title="Python">

```python
# pip install Sympy
from sympy import *  
k,b = symbols('k b')  # 定义符号变量  
sum = simplify(
    (1.0 - (0.0*k+b))**2 +  # (0,1)
    (2.5 - (2.0*k+b))**2 +  # (2,2.5)
    (4.5 - (3.0*k+b))**2 +  # (3,4.5)
    (5.0 - (4.0*k+b))**2 +  # (4,5)
    (6.0 - (5.0*k+b))**2    # (5,6)
)
print(latex(sum))
# 5.0 b^{2} + 28.0 b k - 38.0 b + 54.0 k^{2} - 137.0 k + 88.5
```    

</CodeGroupItem>

<CodeGroupItem title="Matlab">

```matlab
x=[0,2,3,4,5]
y=[1,2.5,4.5,5,6]
plot(x,y,'o')  %% 作散点图
k_b=polyfit(x,y,1)
%% k_b = 1.0338    0.9054
```

</CodeGroupItem>
</CodeGroup>

`D`关于`(k,b)`变化的函数图像如下：  
![最小二乘法最小值](./img/02_statistic/min-value-of-lsm.svg)  

最终曲线的方程为`y=1.0338x+0.9054`，和想象中稍微有些出入，但是随着采样点的增多，该数值会越来越准确。  

### 最小二乘法取最小值  
除了使用`Matlab` 等工具外，还有没有其他办法可以求`D`的最小值呢？分别求`D`关于`k,b`的偏导数，对于这个例子，偏导数全部为`0`时，`D`取到最小值。  
然而对于计算机来说，求数值解要比求解析解更加简单，毕竟计算机就是用来计算的、而不是用来思考的。  

如何求数值解？我们先假设上图中有任意一点`(ki,bi)`，只要让这个点始终沿着`D`减小的方向移动一点点`𝛿`就好了。也就是说应该始终沿着偏导数`<∂k,∂b>`向量相反的方向移动。  
![梯度下降法](./img/02_statistic/gradient-descent.svg)  
其中，`<∂k,∂b>`被称作梯度，完整的写法如下：  
![梯度](./img/02_statistic/gradient.svg)  
这种通过梯度迭代的方法也就叫做**梯度下降法**。也是神经网络中最重要的概念之一。


## 最小二乘法的扩展  
上面的例子中，我们只对单输入单输出的函数进行了拟合，最终得到了一组系数`(k,b)`来确定了一条直线。那么如果是多输入单输出、或者是非直线的函数应该怎么拟合呢？  

### 高维形式
对于二维平面中的点我们可以用一维的线取拟合；对于三维空间中的点我们可以用二维的平面去拟合；对于`n` 维空间中的点，我们则可以用`n-1` 维的超平面去拟合。形式如下：  
![最小二乘法的高维形式](./img/03_lsm/high-dim-lsm.svg)  
其中`xi`作为输入项的集合 ，一般取`k0*x0=k0`作为常数项。  

### 拟合曲线  
即使是高维最小二乘法也只能拟合直线、平面。如果函数的输出并不是分布在一条直线、平面上（如下图），该怎么办呢？
![对数函数散点图](./img/03_lsm/scatter-ln.svg)  

这里用`线性`一词来表示直线变化的情形。我们假设（构造）另一个函数`g(x)=ln(x)`，那么`y~g` 就是线性变化的了。所以对于非线性的数据，我们可以根据数据变化的规律，来构造一个函数，使其非线性化。相当于加了一个线性-非线性转换的环节，然后再进行最小二乘法处理。  

这里的非线性函数一般被称为**激活函数**。人类的神经元一般当外界的信号打到某一个值的时候才会被激活，而且输出与输入并不一定是线性关系。这里看来的话叫做激活函数还是挺形象的。

### 复合函数  
再者，如果函数图像比较复杂，那我们可以通过用一组或者多组简单的函数图像叠加来进行拟合。这里很容易想到级数的概念，事实上道理是一样的。例如：`y=tanh(2sin(x)+1)-tanh(sin(x+1))+1`的图像是下图中最粗的蓝色曲线，它是由一系列简单函数依次传递组成的。  
![复合函数图像](./img/03_lsm/composite-function.svg)  
而对应的函数间的传递方式如下：  
![复合函数分解](./img/03_lsm/composite-function2.svg
)

#### 复合函数求导  
对于复合函数的求导过程，我们有公式：
![复合函数求导](./img/03_lsm/composite-function-gradient.svg
)  
偏导数的求法也是一样，分别求出每一步的导数，最后乘起来就是最终的导数。  

### 误差函数  
前面我们使用了平均值，数值距离的最小值来表示拟合的程度。其实我们还可以选择其他的函数作为误差函数。例如：我们可以选择点到直线的距离、点到平面的距离等等。但是只要保证一条：最终的误差函数在定义域内应该有且只有一个最小值。否则在使用梯度下降时可能会陷入某一个局部最优解。也就是说误差函数`D` 最好像下图的左边而不是右边： 
![误差函数](./img/03_lsm/diff-function.svg)


## 矩阵与行列式  
来看一个鸡兔同笼问题：今有鸡兔同笼，共有头36，脚48，问鸡、兔各几何？易得兔12、鸡24；再问：今笼有俩鸡仨兔，问有头、脚各几何？于是我们可以列出下列表格：  

![矩阵乘法](./img/04_matrix/multiply.svg)

那我们将特征表格AB 按照某种规律进行运算，就能得到最终的规律。先计算头的数量`h=1x2+1x3=5`，再计算脚的数量`f=2x2+4x3=16`。可以总结出，A 的每一行与B 的每一列的对应项相乘之和就是最后的结果，记作矩阵的乘法`C=AB`。  
至于为什么不行列转置，因为这是约定的记法，就像2 记作2 而不是记作3 一样。  

### Numpy中的矩阵运算  
在`numpy` 中定义了一系列的矩阵运算函数，这里简单整理几个常用的，后续再慢慢增加。  
```python
import numpy as np  
x = np.array([1,2,3])  # 定义行向量
# [1, 2, 3]

x = x.reshape(3, -1)  # 重新排列元素  
# 重新排列为3 行矩阵，列数`-1`表示由函数计算决定有多少列  

y=0.5
y=x*y  # 矩阵A*B 表示对应元素相乘
# y 会被自动扩展为尺寸同x，元素全部为0.5 的矩阵  
# 此操作成为广播
# +,-,*,/,power,log,sqrt,log 等运算均适用  

z=x.dot(y.T+1)  # 矩阵乘法  
# 即上面鸡兔同笼问题推导出来的矩阵乘法 
# .T 表示矩阵的转置  
'''z=
[[1.5 2.  2.5]
 [3.  4.  5. ]
 [4.5 6.  7.5]]
'''

print(z.mean(axis=0))  # 输出平均值  
# axis 表示矩阵的维度序号
# [3. 4. 5.]
print(z.mean(axis=1))  # 对照z 的值理解
# [2. 4. 6.]
# 不是太好理解，大致表示对第n 层的元素求平均值

np.zeros((row, col))  # 初始化0 矩阵
np.around(z)  # 四舍五入  
z.flatten()  # 平坦化，按某种顺序转化为行向量，一般是按C 数组的方式  
```







